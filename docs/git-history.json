[
  {
    "hash": "aea7f4a372a6d9f325f51fcd128399a5a3419f01",
    "short_hash": "aea7f4a",
    "author": "=",
    "date": "2026-01-26T22:11:06-05:00",
    "message": "feat: Use backend-integrated quantized tensors (Paulo's approach)\n\nUpdated to use EMLX.quantized_tensor/5 which creates Nx.Tensor with\nquantization_options in the backend struct. This is cleaner than\nthe QuantizedTensor struct approach.\n\nquantized_loader.ex:\n- load_quantized_weight returns Nx.Tensor with backend quant options\n- Uses EMLX.quantized_tensor instead of EMLX.from_quantized\n- count_quantized_params checks EMLX.Backend.quantized?\n\nattention.ex:\n- quantized_linear uses plain Nx.dot\n- Nx.dot automatically dispatches to quantized_matmul\n\nmodel.ex:\n- embedding_lookup checks EMLX.Backend.quantized?\n- Gets scales/biases from quantization_options for dequantize\n\nBenefits of backend-integrated approach:\n- Works with standard Nx operations\n- No wrapper struct needed\n- Quantization info travels with tensor through Nx type system\n- Cleaner integration path for Bumblebee\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 4
  },
  {
    "hash": "e3f98d878796e133682e9f514cf1f13a2183f52c",
    "short_hash": "e3f98d8",
    "author": "=",
    "date": "2026-01-26T22:04:15-05:00",
    "message": "feat: Use EMLX.QuantizedTensor for transparent dispatch\n\nUpdated bobby_posts to use the new transparent quantized tensor API:\n\nquantized_loader.ex:\n- load_quantized_weight now returns EMLX.QuantizedTensor\n- QuantizedTensor bundles weight/scales/biases with metadata\n- Created once at load time, reused on every forward pass\n\nattention.ex:\n- quantized_linear accepts QuantizedTensor, uses EMLX.dot\n- EMLX.dot automatically dispatches to quantized_matmul\n- Legacy map format still supported for backward compatibility\n\nmodel.ex:\n- embedding_lookup accepts QuantizedTensor\n- Uses QuantizedTensor.dequantize helper\n\nBenefits:\n- Cleaner API: no manual from_nx/to_nx on every call\n- Single dispatch point: QuantizedTensor created once\n- Transparent: downstream code doesn't need EMLX-specific calls\n- Path to Bumblebee integration: model definitions unchanged\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 4
  },
  {
    "hash": "404816bb58b82d335ed6aff22f3662d665746466",
    "short_hash": "404816b",
    "author": "=",
    "date": "2026-01-26T21:53:18-05:00",
    "message": "docs: Add comprehensive upstream plans for EMLX and Bumblebee\n\nTwo new documents addressing Paulo Valente's feedback from EMLX PR #95:\n\nemlx-quantization-plan.md:\n- Transparent quantized tensors via EMLX.QuantizedTensor struct\n- Smart Nx.dot override to detect and dispatch quantized operands\n- Comprehensive test plan for quantized tensor operations\n- Migration path from current EMLX-specific to Nx-transparent API\n\nFULL_UPSTREAM_PLAN.md:\n- Complete 3-month upstream strategy across EMLX, Safetensors, Bumblebee\n- Stream 1: EMLX foundation (QuantizedTensor, smart Nx.dot)\n- Stream 2: Safetensors dependency check\n- Stream 3: Bumblebee integration (quantized loading, LoRA adapters)\n- Stream 4: Training integration (mlx_lm wrapper)\n- Test matrix, success criteria, risk mitigation\n\nKey insight: Don't expose EMLX.quantized_matmul directly. Make Nx.dot\nsmart enough to detect quantized tensors and dispatch automatically.\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 2
  },
  {
    "hash": "5fc4799b2f87f1b3c676f734fe403e5adf85e1b0",
    "short_hash": "5fc4799",
    "author": "=",
    "date": "2026-01-26T08:50:25-05:00",
    "message": "docs: Flesh out decision graph for Bumblebee contribution\n\nAdded 51 new nodes capturing:\n- 3 key decisions (backend strategy, model definition, LoRA scope)\n- 9 options for those decisions with rationale\n- 25 observations about implementation details\n- 5-step upstream contribution plan\n- API design decisions for loader and adapters\n- Technical details (LoRA scaling, weight format, KV cache)\n- User workflow and performance expectations\n\nGraph now: 213 nodes, 235 edges\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 1
  },
  {
    "hash": "858f3841f0971d03df68d079cda3eaa3e44e7556",
    "short_hash": "858f384",
    "author": "=",
    "date": "2026-01-25T18:06:45-05:00",
    "message": "chore: Link to new bobby_posts_adapters repo\n\n- Create separate Git LFS repo for adapter weights\n- Update all paths to point to new adapters repo location\n- Add adapters repo reference to README\n- Document clone instructions with git lfs\n\nAdapters repo: https://github.com/notactuallytreyanastasio/bobby_posts_adapters\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 5
  },
  {
    "hash": "07fddf7ddcb3dc2b93963b35304586ccbab7263f",
    "short_hash": "07fddf7",
    "author": "=",
    "date": "2026-01-24T03:18:18-05:00",
    "message": "chore: Add TODO comments for future upstream deps\n\nOnce PRs are merged:\n- EMLX: quantization ops PR\n- Safetensors: hex.pm publication\n- Bumblebee: already has Qwen3 in upstream\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 1
  },
  {
    "hash": "e6211806bf6e5a69c5ba2433c169fc57f7408ee0",
    "short_hash": "e621180",
    "author": "=",
    "date": "2026-01-24T03:14:08-05:00",
    "message": "docs: Add blog post and upstream PR plan\n\n- BLOG_POST.md: Comprehensive writeup of the pure Elixir journey\n- UPSTREAM_PLAN.md: Plan for PR'ing to EMLX, publishing safetensors\n\nTracked in deciduous as nodes 35-39.\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 2
  },
  {
    "hash": "9471f996dba1ea2c59b8bda53ae2f1258785d16d",
    "short_hash": "9471f99",
    "author": "=",
    "date": "2026-01-24T01:49:49-05:00",
    "message": "feat: Pure Elixir LoRA inference - no Python\n\nMajor changes:\n- Switch from fused model to runtime LoRA for better quality\n  - Fused model re-quantizes after merge, losing precision\n  - Runtime LoRA keeps adapters in fp32, preserving fine-tuning\n- Replace Python subprocess tokenization with Bumblebee tokenizer\n  - New BobbyPosts.Tokenizer module using Bumblebee.load_tokenizer\n  - Pure Elixir encode/decode via Tokenizers library\n- Add /no_think directive to disable Qwen3 chain-of-thought\n- Strip emojis and hashtags from generated posts\n- Clean up response extraction for Bumblebee's token format\n\nThe LoRA formula (attention.ex:179-193):\n  base_output = quantized_matmul(x, weights)  # 4-bit GPU\n  lora_output = (x @ lora_a @ lora_b) * scale # fp32 precise\n  return base_output + lora_output\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 8
  },
  {
    "hash": "21f44eb56c76c4cbc56997cd16d971c2a2e030f4",
    "short_hash": "21f44eb",
    "author": "YOUR_NAME_HERE",
    "date": "2026-01-22T16:17:48-05:00",
    "message": "fix: Add temperature sampling to eliminate deterministic cat posts\n\n- Fixed tokenization to use temp files instead of string interpolation\n  which was causing newlines to be escaped incorrectly\n- Added temperature-based sampling with top-p filtering\n- Default temperature=0.7, top_p=0.9 matches mlx-lm behavior\n- Greedy decoding (temperature=0) deterministically produces cat posts\n- Sampling produces varied output matching the training data distribution\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
    "files_changed": 2
  }
]